

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ROS Package bob_llm &mdash; Bob&#39;s Handbuch 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=0603aa02" />

  
    <link rel="shortcut icon" href="../../../_static/profile_image-300x300.png"/>
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Bob's Handbuch
              <img src="../../../_static/bob_vv.gif" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Packages</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bob-coquitts.html">ROS Package bob_coquitts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-flux2.html">ROS Package bob_flux2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-flux2k.html">ROS Package bob_flux2k</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-launch.html">ROS Package bob_launch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-llm.html">ROS Package bob_llm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-moondream.html">ROS Package bob_moondream</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-moondream-msgs.html">ROS Package bob_moondream_msgs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-msgs.html">ROS Package bob_msgs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-sd35.html">ROS Package bob_sd35</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-topic-tools.html">ROS Package bob_topic_tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bob-vector-db.html">ROS Package bob_vector_db</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../voskros.html">ROS Package VoskRos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">OTHER</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vox.html">Package VOX Real-time Transcription</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autoapi/index.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Bob's Handbuch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ROS Package bob_llm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/bob/src/bob_llm/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ros-package-bob-llm">
<h1>ROS Package <a class="reference external" href="https://github.com/bob-ros2/bob_llm">bob_llm</a><a class="headerlink" href="#ros-package-bob-llm" title="Link to this heading"></a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">bob_llm</span></code> package provides a ROS 2 node (<code class="docutils literal notranslate"><span class="pre">llm</span> <span class="pre">node</span></code>) that acts as a powerful interface to an external Large Language Model (LLM). It operates as a stateful service that maintains a conversation, connects to any OpenAI-compatible API, and features a robust tool execution system.</p>
<section id="features">
<h2>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>OpenAI-Compatible:</strong> Connects to any LLM backend that exposes an OpenAI-compatible API endpoint (e.g., <code class="docutils literal notranslate"><span class="pre">Ollama</span></code>, <code class="docutils literal notranslate"><span class="pre">vLLM</span></code>, <code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code>, commercial APIs).</p></li>
<li><p><strong>Stateful Conversation:</strong> Maintains chat history to provide conversational context to the LLM.</p></li>
<li><p><strong>Dynamic Tool System:</strong> Dynamically loads Python functions from user-provided files and makes them available to the LLM. The LLM can request to call these functions to perform actions or gather information.</p></li>
<li><p><strong>Streaming Support:</strong> Can stream the LLM’s final response token-by-token for real-time feedback.</p></li>
<li><p><strong>Fully Parameterized:</strong> All configuration, from API endpoints to LLM generation parameters, is handled through a single ROS parameters file.</p></li>
<li><p><strong>Multi-modality:</strong> Supports multimodal input (e.g., images) via JSON prompts.</p></li>
<li><p><strong>Lightweight:</strong> The node is simple and has minimal dependencies, requiring only a few standard Python libraries (<code class="docutils literal notranslate"><span class="pre">requests</span></code>, <code class="docutils literal notranslate"><span class="pre">PyYAML</span></code>) on top of ROS 2.</p></li>
</ul>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p><strong>Clone the Repository</strong>
Navigate to your ROS 2 workspace’s <code class="docutils literal notranslate"><span class="pre">src</span></code> directory and clone the repository:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~/ros2_ws/src
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/bob-ros2/bob_llm.git
</pre></div>
</div>
</li>
<li><p><strong>Install Dependencies</strong>
The node requires a few Python packages. It is recommended to install these within a virtual environment.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>requests<span class="w"> </span>PyYAML
</pre></div>
</div>
<p>The required ROS 2 dependencies (<code class="docutils literal notranslate"><span class="pre">rclpy</span></code>, <code class="docutils literal notranslate"><span class="pre">std_msgs</span></code>) will be resolved by the build system.</p>
</li>
<li><p><strong>Build the Workspace</strong>
Navigate to the root of your workspace and build the package:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~/ros2_ws
colcon<span class="w"> </span>build<span class="w"> </span>--packages-select<span class="w"> </span>bob_llm
</pre></div>
</div>
</li>
<li><p><strong>Source the Workspace</strong>
Before running the node, remember to source your workspace’s setup file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span>install/setup.bash
</pre></div>
</div>
</li>
</ol>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h2>
<section id="run-the-node">
<h3>1. Run the Node<a class="headerlink" href="#run-the-node" title="Link to this heading"></a></h3>
<p>Before running, ensure your LLM server is active and the <code class="docutils literal notranslate"><span class="pre">api_url</span></code> in your params file is correct.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make sure your workspace is sourced</span>
<span class="c1"># source install/setup.bash</span>

<span class="c1"># Run the node with your parameters file</span>
ros2<span class="w"> </span>run<span class="w"> </span>bob_llm<span class="w"> </span>llm<span class="w"> </span>--ros-args<span class="w"> </span>--params-file<span class="w"> </span>/path/to/your/ros2_ws/src/bob_llm/config/node_params.yaml
</pre></div>
</div>
</section>
<section id="interact-with-the-node">
<h3>2. Interact with the Node<a class="headerlink" href="#interact-with-the-node" title="Link to this heading"></a></h3>
<p>The package includes a convenient helper script, <code class="docutils literal notranslate"><span class="pre">scripts/query.sh</span></code>, for interacting with the node directly from the command line.</p>
<p>Once the <code class="docutils literal notranslate"><span class="pre">llm</span> <span class="pre">node</span></code> is running, open a new terminal (with the workspace sourced) and run the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ros2<span class="w"> </span>run<span class="w"> </span>bob_llm<span class="w"> </span>query.sh
---<span class="w"> </span>Listening<span class="w"> </span><span class="k">for</span><span class="w"> </span>results<span class="w"> </span>on<span class="w"> </span>llm_response<span class="w"> </span>---
---<span class="w"> </span>Enter<span class="w"> </span>your<span class="w"> </span>prompt<span class="w"> </span>below<span class="w"> </span><span class="o">(</span>Press<span class="w"> </span>Ctrl+C<span class="w"> </span>to<span class="w"> </span><span class="nb">exit</span><span class="o">)</span><span class="w"> </span>---
&gt;<span class="w"> </span>What<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>status<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>robot?
Robot<span class="w"> </span>status:<span class="w"> </span>Battery<span class="w"> </span>is<span class="w"> </span>at<span class="w"> </span><span class="m">85</span>%.<span class="w"> </span>All<span class="w"> </span>systems<span class="w"> </span>are<span class="w"> </span>nominal.<span class="w"> </span>Currently<span class="w"> </span>idle.
&gt;
</pre></div>
</div>
</section>
<section id="advanced-input-multi-modality">
<h3>3. Advanced Input &amp; Multi-modality<a class="headerlink" href="#advanced-input-multi-modality" title="Link to this heading"></a></h3>
<p>The node supports advanced input formats beyond simple text strings. If the input message on <code class="docutils literal notranslate"><span class="pre">/llm_prompt</span></code> is a valid JSON string, it is parsed and treated as a message object.</p>
<p><strong>Generic JSON Input:</strong>
You can pass any valid JSON dictionary. If it contains a <code class="docutils literal notranslate"><span class="pre">role</span></code> field (e.g., <code class="docutils literal notranslate"><span class="pre">user</span></code>), it is treated as a standard message object and appended to the history. This allows you to send custom content structures supported by your specific LLM backend (e.g., complex multimodal inputs, custom fields).</p>
<p><strong>Image Handling Helper:</strong>
For convenience, the node includes a helper for handling images. If <code class="docutils literal notranslate"><span class="pre">process_image_urls</span></code> is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the node looks for an <code class="docutils literal notranslate"><span class="pre">image_url</span></code> field in your JSON input. It will automatically fetch the image (from <code class="docutils literal notranslate"><span class="pre">file://</span></code> or <code class="docutils literal notranslate"><span class="pre">http://</span></code> URLs), base64 encode it, and format the message according to the OpenAI Vision API specification.</p>
<p><strong>Example (Image Helper):</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2<span class="w"> </span>topic<span class="w"> </span>pub<span class="w"> </span>/llm_prompt<span class="w"> </span>std_msgs/msg/String<span class="w"> </span><span class="s2">&quot;data: &#39;{\&quot;role\&quot;: \&quot;user\&quot;, \&quot;content\&quot;: \&quot;Describe this image\&quot;, \&quot;image_url\&quot;: \&quot;file:///path/to/image.jpg\&quot;}&#39;&quot;</span><span class="w"> </span>-1
</pre></div>
</div>
</section>
</section>
<section id="conversation-flow">
<h2>Conversation Flow<a class="headerlink" href="#conversation-flow" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>A user publishes a prompt to the <code class="docutils literal notranslate"><span class="pre">/llm_prompt</span></code> topic.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">llm</span> <span class="pre">node</span></code> adds the prompt to its internal chat history.</p></li>
<li><p>The node sends the history and a list of available tools to the LLM backend.</p></li>
<li><p>The LLM decides whether to respond directly or use a tool.</p>
<ul class="simple">
<li><p><strong>If Tool:</strong> The LLM returns a request to call a specific function. The <code class="docutils literal notranslate"><span class="pre">llm</span> <span class="pre">node</span></code> executes the function, appends the result to the history, and sends the updated history back to the LLM. This loop can repeat multiple times.</p></li>
<li><p><strong>If Text:</strong> The LLM generates a final, natural language response.</p></li>
</ul>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">llm</span> <span class="pre">node</span></code> publishes the final response. If streaming is enabled, it’s sent token-by-token to <code class="docutils literal notranslate"><span class="pre">/llm_stream</span></code> and the full message is sent to <code class="docutils literal notranslate"><span class="pre">/llm_response</span></code> upon completion. Otherwise, the full response is sent directly to <code class="docutils literal notranslate"><span class="pre">/llm_response</span></code>.</p></li>
</ol>
</section>
<section id="ros-2-api">
<h2>ROS 2 API<a class="headerlink" href="#ros-2-api" title="Link to this heading"></a></h2>
<p>The node uses the following topics for communication:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Topic</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/llm_prompt</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">std_msgs/msg/String</span></code></p></td>
<td><p><strong>(Subscribed)</strong> Receives user prompts to be processed by the LLM.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/llm_response</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">std_msgs/msg/String</span></code></p></td>
<td><p><strong>(Published)</strong> Publishes the final, complete response from the LLM.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/llm_stream</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">std_msgs/msg/String</span></code></p></td>
<td><p><strong>(Published)</strong> Publishes token-by-token chunks of the LLM’s response if streaming is enabled.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/llm_latest_turn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">std_msgs/msg/String</span></code></p></td>
<td><p><strong>(Published)</strong> Publishes the latest user/assistant turn as a JSON string: <code class="docutils literal notranslate"><span class="pre">[{&quot;role&quot;:</span> <span class="pre">&quot;user&quot;,</span> <span class="pre">...},</span> <span class="pre">{&quot;role&quot;:</span> <span class="pre">&quot;assistant&quot;,</span> <span class="pre">...}]</span></code>.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Link to this heading"></a></h2>
<p>The node is configured entirely through a ROS parameters YAML file (e.g., <code class="docutils literal notranslate"><span class="pre">config/node_params.yaml</span></code>).</p>
<section id="ros-parameters">
<h3>ROS Parameters<a class="headerlink" href="#ros-parameters" title="Link to this heading"></a></h3>
<p>All parameters can be set via a YAML file, command-line arguments, or environment variables. The order of precedence is: command-line arguments &gt; parameters file &gt; environment variables &gt; coded defaults. For array-type parameters, environment variables should be comma-separated strings (e.g., <code class="docutils literal notranslate"><span class="pre">LLM_STOP=&quot;stop1,stop2&quot;</span></code>).</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">api_type</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openai_compatible</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_API_TYPE</span></code></p></td>
<td><p>The type of the LLM backend API. Currently only <code class="docutils literal notranslate"><span class="pre">openai_compatible</span></code> is supported.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">api_url</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">http://localhost:8000</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_API_URL</span></code></p></td>
<td><p>The base URL of the LLM backend. The node appends <code class="docutils literal notranslate"><span class="pre">/v1/chat/completions</span></code> automatically.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">api_key</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">no_key</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_API_KEY</span></code></p></td>
<td><p>The API key (Bearer token) for authentication with the LLM backend, if required.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">api_model</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_API_MODEL</span></code></p></td>
<td><p>The specific model name to use (e.g., “gpt-4”, “llama3”).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">api_timeout</span></code></p></td>
<td><p>double</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">120.0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_API_TIMEOUT</span></code></p></td>
<td><p>Timeout in seconds for API requests to the LLM backend.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">system_prompt</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_SYSTEM_PROMPT</span></code></p></td>
<td><p>The system prompt to set the LLM’s context, personality, and instructions.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">initial_messages_json</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_INITIAL_MESSAGES_JSON</span></code></p></td>
<td><p>A JSON string of initial messages for few-shot prompting to guide the LLM.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">max_history_length</span></code></p></td>
<td><p>integer</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">10</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_MAX_HISTORY_LENGTH</span></code></p></td>
<td><p>Maximum number of user/assistant conversational turns to keep in history.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">message_log</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_MESSAGE_LOG</span></code></p></td>
<td><p>If set to a file path, appends each conversational turn to a persistent JSON log file.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">stream</span></code></p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_STREAM</span></code></p></td>
<td><p>Enable or disable streaming for the final LLM response.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">process_image_urls</span></code></p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_PROCESS_IMAGE_URLS</span></code></p></td>
<td><p>If true, processes <code class="docutils literal notranslate"><span class="pre">image_url</span></code> in JSON prompts by base64 encoding the image.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">response_format</span></code></p></td>
<td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_RESPONSE_FORMAT</span></code></p></td>
<td><p>JSON string defining the output format. See <a class="reference internal" href="#structured-json-output"><span class="xref myst">Structured JSON Output</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">max_tool_calls</span></code></p></td>
<td><p>integer</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">5</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_MAX_TOOL_CALLS</span></code></p></td>
<td><p>Maximum number of consecutive tool calls before aborting to prevent loops.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code></p></td>
<td><p>double</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.7</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_TEMPERATURE</span></code></p></td>
<td><p>Controls the randomness of the output. Lower is more deterministic.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">top_p</span></code></p></td>
<td><p>double</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1.0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_TOP_P</span></code></p></td>
<td><p>Nucleus sampling. Controls output diversity. Alter this or temperature, not both.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code></p></td>
<td><p>integer</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_MAX_TOKENS</span></code></p></td>
<td><p>Maximum number of tokens to generate. <code class="docutils literal notranslate"><span class="pre">0</span></code> means use the server’s default limit.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">stop</span></code></p></td>
<td><p>string array</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[&quot;stop_llm&quot;]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_STOP</span></code></p></td>
<td><p>A list of sequences where the API will stop generating further tokens. Also, sending any of these strings to <code class="docutils literal notranslate"><span class="pre">llm_prompt</span></code> cancels the current generation.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">presence_penalty</span></code></p></td>
<td><p>double</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_PRESENCE_PENALTY</span></code></p></td>
<td><p>Penalizes new tokens based on whether they appear in the text so far.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">frequency_penalty</span></code></p></td>
<td><p>double</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_FREQUENCY_PENALTY</span></code></p></td>
<td><p>Penalizes new tokens based on their existing frequency in the text so far.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tool_interfaces</span></code></p></td>
<td><p>string array</p></td>
<td><p>Path to <code class="docutils literal notranslate"><span class="pre">example_interface.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LLM_TOOL_INTERFACES</span></code></p></td>
<td><p>A list of absolute paths to Python files containing tool functions. The node will attempt to load functions from each file path provided.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="structured-json-output">
<h3>Structured JSON Output<a class="headerlink" href="#structured-json-output" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">response_format</span></code> parameter enables structured output from the LLM, forcing it to respond with valid JSON that conforms to a specified schema. This is useful for parsing responses programmatically, building automation pipelines, or ensuring consistent output formats.</p>
<section id="json-object-mode">
<h4>JSON Object Mode<a class="headerlink" href="#json-object-mode" title="Link to this heading"></a></h4>
<p>The simplest form forces the LLM to output valid JSON:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">llm</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ros__parameters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">response_format</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;{&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;json_object&quot;}&#39;</span>
<span class="w">    </span><span class="nt">system_prompt</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;You</span><span class="nv"> </span><span class="s">are</span><span class="nv"> </span><span class="s">a</span><span class="nv"> </span><span class="s">robot</span><span class="nv"> </span><span class="s">assistant.</span><span class="nv"> </span><span class="s">Always</span><span class="nv"> </span><span class="s">respond</span><span class="nv"> </span><span class="s">with</span><span class="nv"> </span><span class="s">JSON</span><span class="nv"> </span><span class="s">containing</span><span class="nv"> </span><span class="s">&#39;action&#39;</span><span class="nv"> </span><span class="s">and</span><span class="nv"> </span><span class="s">&#39;reasoning&#39;</span><span class="nv"> </span><span class="s">fields.&quot;</span>
</pre></div>
</div>
<blockquote>
<div><p><strong>Important:</strong> When using <code class="docutils literal notranslate"><span class="pre">json_object</span></code> mode, your system prompt or user message <strong>must</strong> mention the word “JSON” — most LLM backends require this or will reject the request.</p>
</div></blockquote>
</section>
<section id="json-schema-mode-strict">
<h4>JSON Schema Mode (Strict)<a class="headerlink" href="#json-schema-mode-strict" title="Link to this heading"></a></h4>
<p>For more control, you can define an exact JSON schema that the LLM must follow. This is supported by OpenAI and compatible backends like vLLM:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">llm</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ros__parameters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">response_format</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">      </span><span class="no">{</span>
<span class="w">        </span><span class="no">&quot;type&quot;: &quot;json_schema&quot;,</span>
<span class="w">        </span><span class="no">&quot;json_schema&quot;: {</span>
<span class="w">          </span><span class="no">&quot;name&quot;: &quot;robot_command&quot;,</span>
<span class="w">          </span><span class="no">&quot;strict&quot;: true,</span>
<span class="w">          </span><span class="no">&quot;schema&quot;: {</span>
<span class="w">            </span><span class="no">&quot;type&quot;: &quot;object&quot;,</span>
<span class="w">            </span><span class="no">&quot;properties&quot;: {</span>
<span class="w">              </span><span class="no">&quot;action&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;move&quot;, &quot;stop&quot;, &quot;rotate&quot;]},</span>
<span class="w">              </span><span class="no">&quot;target&quot;: {&quot;type&quot;: &quot;string&quot;},</span>
<span class="w">              </span><span class="no">&quot;speed&quot;: {&quot;type&quot;: &quot;number&quot;}</span>
<span class="w">            </span><span class="no">},</span>
<span class="w">            </span><span class="no">&quot;required&quot;: [&quot;action&quot;]</span>
<span class="w">          </span><span class="no">}</span>
<span class="w">        </span><span class="no">}</span>
<span class="w">      </span><span class="no">}</span>
</pre></div>
</div>
</section>
<section id="example-command-parsing">
<h4>Example: Command Parsing<a class="headerlink" href="#example-command-parsing" title="Link to this heading"></a></h4>
<p>With the above schema, the LLM will always respond with valid JSON:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ros2<span class="w"> </span>topic<span class="w"> </span>pub<span class="w"> </span>/llm_prompt<span class="w"> </span>std_msgs/msg/String<span class="w"> </span><span class="s2">&quot;data: &#39;Move forward slowly&#39;&quot;</span><span class="w"> </span>-1
</pre></div>
</div>
<p>Response on <code class="docutils literal notranslate"><span class="pre">/llm_response</span></code>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;action&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;move&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;forward&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;speed&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.3</span><span class="p">}</span>
</pre></div>
</div>
<p>This makes it easy to parse the response in downstream nodes without complex text parsing.</p>
<blockquote>
<div><p>[!WARNING]
<strong>llama.cpp Compatibility:</strong> The <code class="docutils literal notranslate"><span class="pre">response_format</span></code> parameter works with llama.cpp using the format <code class="docutils literal notranslate"><span class="pre">{&quot;type&quot;:</span> <span class="pre">&quot;json_object&quot;,</span> <span class="pre">&quot;schema&quot;:</span> <span class="pre">{...}}</span></code>. However, there is a <strong>bug</strong> in llama.cpp server when combining <code class="docutils literal notranslate"><span class="pre">response_format</span></code> with tool calls — it crashes with <code class="docutils literal notranslate"><span class="pre">Content</span> <span class="pre">path</span> <span class="pre">must</span> <span class="pre">be</span> <span class="pre">a</span> <span class="pre">string</span></code>. If you need both features, consider using vLLM or another backend until this is fixed upstream.</p>
</div></blockquote>
</section>
</section>
<section id="conversation-logging">
<h3>Conversation Logging<a class="headerlink" href="#conversation-logging" title="Link to this heading"></a></h3>
<p>The node can optionally save the entire conversation to a JSON file, which is useful for debugging, analysis, or creating datasets for fine-tuning models.</p>
<p>To enable logging, set the <code class="docutils literal notranslate"><span class="pre">message_log</span></code> parameter to an absolute file path (e.g., <code class="docutils literal notranslate"><span class="pre">/home/user/conversation.json</span></code>). The node will append each user prompt and the corresponding assistant response to this file. If the file does not exist, it will be created. On the first write to a new file, the <code class="docutils literal notranslate"><span class="pre">system_prompt</span></code> (if configured) will be automatically added as the first entry.</p>
<p>The resulting file will be a flat JSON array of message objects, like this:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;You are a helpful robot assistant.&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;What is the status of the robot?&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Robot status: Battery is at 85%. All systems are nominal. Currently idle.&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="tool-system">
<h2>Tool System<a class="headerlink" href="#tool-system" title="Link to this heading"></a></h2>
<p>The standout feature of this node is its ability to use dynamically loaded Python functions as tools. The LLM can request to call these functions to perform actions or gather information.</p>
<section id="creating-a-tool-file">
<h3>Creating a Tool File<a class="headerlink" href="#creating-a-tool-file" title="Link to this heading"></a></h3>
<p>A tool file is a standard Python script containing one or more functions. The system automatically generates the necessary API schema for the LLM from your function’s signature (including type hints) and its docstring. The first line of the docstring is used as the function’s description for the LLM.</p>
<p><strong>Example: <code class="docutils literal notranslate"><span class="pre">config/example_interface.py</span></code></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_weather</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">unit</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;celsius&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the current weather in a given location.</span>

<span class="sd">    This is an example function and will return a fixed string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;tokyo&quot;</span> <span class="ow">in</span> <span class="n">location</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;The weather in Tokyo is 10 degrees </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2"> and sunny.&quot;</span>
    <span class="k">elif</span> <span class="s2">&quot;san francisco&quot;</span> <span class="ow">in</span> <span class="n">location</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;The weather in San Francisco is 15 degrees </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2"> and foggy.&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Sorry, I don&#39;t have the weather for </span><span class="si">{</span><span class="n">location</span><span class="si">}</span><span class="s2">.&quot;</span>

<span class="k">def</span> <span class="nf">get_robot_status</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieves the current status of the robot.</span>

<span class="sd">    This function checks the robot&#39;s battery level, joint states, and current task.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># In a real scenario, this would query robot topics or services</span>
    <span class="k">return</span> <span class="s2">&quot;Robot status: Battery is at 85%. All systems are nominal. Currently idle.&quot;</span>
</pre></div>
</div>
</section>
<section id="configuring-tools">
<h3>Configuring Tools<a class="headerlink" href="#configuring-tools" title="Link to this heading"></a></h3>
<p>To make your tools available to the LLM, you must provide a list of absolute paths to your Python tool files in the <code class="docutils literal notranslate"><span class="pre">tool_interfaces</span></code> parameter.</p>
<p>Open <code class="docutils literal notranslate"><span class="pre">config/node_params.yaml</span></code> and edit the <code class="docutils literal notranslate"><span class="pre">tool_interfaces</span></code> list. You must replace any placeholder path with the full, absolute path to the tool file on your system.</p>
<p>For example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># In config/node_params.yaml</span>
<span class="nt">llm</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ros__parameters</span><span class="p">:</span>
<span class="w">    </span><span class="c1"># ... other parameters</span>

<span class="w">    </span><span class="c1"># A list of Python modules to load as tool interfaces.</span>
<span class="w">    </span><span class="c1"># Replace the path below with the absolute path on your machine.</span>
<span class="w">    </span><span class="nt">tool_interfaces</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;/home/user/ros2_ws/src/bob_llm/config/example_interface.py&quot;</span>
<span class="w">      </span><span class="c1"># You can add more tool files here</span>
<span class="w">      </span><span class="c1"># - &quot;/home/user/ros2_ws/src/my_robot_tools/my_robot_tools/tools.py&quot;</span>

<span class="w">    </span><span class="c1"># ... other parameters</span>
</pre></div>
</div>
</section>
<section id="inbuilt-tools">
<h3>Inbuilt Tools<a class="headerlink" href="#inbuilt-tools" title="Link to this heading"></a></h3>
<p>The package comes with several ready-to-use tool modules in the <code class="docutils literal notranslate"><span class="pre">config/</span></code> directory.</p>
<section id="ros-cli-tools-config-ros-cli-tools-py">
<h4>1. ROS CLI Tools (<code class="docutils literal notranslate"><span class="pre">config/ros_cli_tools.py</span></code>)<a class="headerlink" href="#ros-cli-tools-config-ros-cli-tools-py" title="Link to this heading"></a></h4>
<p>This module provides a comprehensive set of tools that wrap standard ROS 2 command-line interface (CLI) functionalities. It allows the LLM to inspect the system (list nodes, topics, services) and interact with it (publish messages, call services, get/set parameters).</p>
<p><strong>Dependencies:</strong></p>
<ul class="simple">
<li><p>None (uses standard ROS 2 libraries and CLI tools).</p></li>
</ul>
<p><strong>Usage:</strong>
Add the absolute path to <code class="docutils literal notranslate"><span class="pre">config/ros_cli_tools.py</span></code> to your <code class="docutils literal notranslate"><span class="pre">tool_interfaces</span></code> parameter.</p>
</section>
<section id="qdrant-memory-tools-config-qdrant-tools-py">
<h4>2. Qdrant Memory Tools (<code class="docutils literal notranslate"><span class="pre">config/qdrant_tools.py</span></code>)<a class="headerlink" href="#qdrant-memory-tools-config-qdrant-tools-py" title="Link to this heading"></a></h4>
<p>This module enables long-term memory for the LLM using the Qdrant vector database. It uses the Model Context Protocol (MCP) to communicate with a Qdrant MCP server.</p>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">save_memory</span></code>: Stores information with optional metadata.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">search_memory</span></code>: Semantically searches for relevant information in the database.</p></li>
</ul>
<p><strong>Prerequisites:</strong></p>
<ol class="arabic">
<li><p><strong>Qdrant Server:</strong> You must have a running Qdrant server instance. See <a class="reference external" href="https://qdrant.tech/">Qdrant Quickstart</a> for installation instructions.</p></li>
<li><p><strong>MCP Python Package:</strong> Install the <code class="docutils literal notranslate"><span class="pre">mcp</span></code> library:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mcp
</pre></div>
</div>
</li>
<li><p><strong>Qdrant MCP Server:</strong> Install the Qdrant MCP server (see <a class="reference external" href="https://github.com/qdrant/mcp-server-qdrant">mcp-server-qdrant</a>). Ensure <code class="docutils literal notranslate"><span class="pre">mcp-server-qdrant</span></code> is in your PATH.</p></li>
</ol>
<p><strong>Environment Variables:</strong>
The <code class="docutils literal notranslate"><span class="pre">qdrant_tools.py</span></code> module requires the following environment variables to connect to your Qdrant instance:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">QDRANT_URL</span><span class="o">=</span><span class="s2">&quot;http://localhost:6333&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">QDRANT_API_KEY</span><span class="o">=</span><span class="s2">&quot;your_key&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">COLLECTION_NAME</span><span class="o">=</span><span class="s2">&quot;my_knowledge_base&quot;</span>
</pre></div>
</div>
<p><strong>Usage:</strong>
Add the absolute path to <code class="docutils literal notranslate"><span class="pre">config/qdrant_tools.py</span></code> to your <code class="docutils literal notranslate"><span class="pre">tool_interfaces</span></code> parameter.
The node will load all specified tool files at startup.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright BobRos.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>